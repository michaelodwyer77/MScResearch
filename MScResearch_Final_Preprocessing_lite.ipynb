{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelodwyer77/MScResearch/blob/main/MScResearch_Final_Preprocessing_lite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AhPXuOUOLPpe"
      },
      "id": "AhPXuOUOLPpe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "RdWOH2wTHRm8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "1017f89d-0a2b-4c9f-f7b8-caf7a0d8ec32"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-43e47270-33ec-41a0-a75d-5cd26a155224\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-43e47270-33ec-41a0-a75d-5cd26a155224\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving vehicles_lite.csv to vehicles_lite.csv\n"
          ]
        }
      ],
      "id": "RdWOH2wTHRm8"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a710481b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a710481b",
        "outputId": "faccec21-af15-4209-e867-6e1b52d8e5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           id                                                url      region  \\\n",
            "0  7316878036  https://bham.craigslist.org/ctd/d/birmingham-2...  birmingham   \n",
            "1  7316874816  https://bham.craigslist.org/ctd/d/birmingham-2...  birmingham   \n",
            "2  7316873897  https://bham.craigslist.org/cto/d/helena-2001-...  birmingham   \n",
            "3  7316872263  https://bham.craigslist.org/ctd/d/birmingham-2...  birmingham   \n",
            "4  7316871664  https://bham.craigslist.org/ctd/d/birmingham-2...  birmingham   \n",
            "\n",
            "                    region_url  price    year manufacturer  \\\n",
            "0  https://bham.craigslist.org  21950  2012.0       toyota   \n",
            "1  https://bham.craigslist.org  13950  2011.0       toyota   \n",
            "2  https://bham.craigslist.org   5900  2001.0          NaN   \n",
            "3  https://bham.craigslist.org  12950  2005.0          bmw   \n",
            "4  https://bham.craigslist.org  18950  2010.0         ford   \n",
            "\n",
            "                       model  condition    cylinders  ... Unnamed: 30  \\\n",
            "0                 tacoma 4x4  excellent  4 cylinders  ...         NaN   \n",
            "1                     tacoma       good  4 cylinders  ...         NaN   \n",
            "2                      F-350        NaN  8 cylinders  ...         NaN   \n",
            "3                         z4  excellent  6 cylinders  ...         NaN   \n",
            "4  f150 lariat supercrew 4x4       good  8 cylinders  ...         NaN   \n",
            "\n",
            "   Unnamed: 31 Unnamed: 32 Unnamed: 33 Unnamed: 34 Unnamed: 35 Unnamed: 36  \\\n",
            "0          NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1          NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2          NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3          NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4          NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "  Unnamed: 37 Unnamed: 38 Unnamed: 39  \n",
            "0         NaN         NaN         NaN  \n",
            "1         NaN         NaN         NaN  \n",
            "2         NaN         NaN         NaN  \n",
            "3         NaN         NaN         NaN  \n",
            "4         NaN         NaN         NaN  \n",
            "\n",
            "[5 rows x 40 columns]\n",
            "1859\n"
          ]
        }
      ],
      "source": [
        "#This vehicles_lite dataset is already classifed as HIGH_PRICE, MEDIUM_PRICE\n",
        "import pandas as pd\n",
        "\n",
        "dataframe_adverts = pd.read_csv('vehicles_lite.csv',encoding='utf-8')\n",
        "print(dataframe_adverts.head())\n",
        "print(len(dataframe_adverts))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4KIVeoJCn-l8"
      },
      "id": "4KIVeoJCn-l8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a50b7144",
      "metadata": {
        "id": "a50b7144"
      },
      "source": [
        "# Pre-processing - cleanup, stopword removal, tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bc8c412c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc8c412c",
        "outputId": "8cb0b391-0f4e-48b0-f821-00a4202b85b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is corpus_small_list....\n",
            "1000    [morris, auto, sales, valleydale, roadbirmingh...\n",
            "1001    [morris, auto, sales, valleydale, roadbirmingh...\n",
            "1002    [chevrolet, corvette, conv, offered, toy, stor...\n",
            "1003    [chevrolet, corvette, coupe, offered, toy, sto...\n",
            "1004    [chevrolet, corvette, grand, sport, offered, t...\n",
            "                              ...                        \n",
            "1495    [selling, nice, ram, laramie, mega, cab, deise...\n",
            "1496    [honda, civic, runs, drives, good, vtec, cyl, ...\n",
            "1497    [chevy, trailblazer, alloy, wheels, airbags, a...\n",
            "1498    [truckcountryautonetselling, awesome, ford, fu...\n",
            "1499    [awesome, toyota, runner, trd, loaded, low, mi...\n",
            "Name: description, Length: 500, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Clean text\n",
        "def cleanText(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('@', '', text)\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub(r\"[^a-zA-Z ]+\", \"\", text)\n",
        "\n",
        "    return text\n",
        "# Tokenize Text\n",
        "def tokenizeText(text):\n",
        "    oStopWords = stopwords.words('english')\n",
        "    text = cleanText(text)\n",
        "    #Tokenize the data\n",
        "    text = nltk.word_tokenize(text)\n",
        "    #Remove stopwords\n",
        "    text = [w for w in text if w not in oStopWords]\n",
        "    return text\n",
        "\n",
        "\n",
        "#aList = dataframe_adverts['description'].tolist()\n",
        "\n",
        "adverts_description_preprocessed_str_list = []\n",
        "#corpus_original_list  = [str(i) for i in dataframe_adverts['description']]\n",
        "adverts_description_preprocessed_str_list = dataframe_adverts['description'].apply(lambda sText: tokenizeText(sText))\n",
        "corpus_small_list = str(adverts_description_preprocessed_str_list[1000:1500])\n",
        "print(\"this is corpus_small_list....\")\n",
        "#print(corpus_small_str)\n",
        "print(str(corpus_small_list))\n",
        "\n",
        "# #corpus_original_list  = [str(i) for i in adverts_description_preprocessed_str_list]\n",
        "# print(\"This is corpus small joined.....\")\n",
        "# corpus_small = ' '.join(corpus_small_str)\n",
        "# print(corpus_small)\n",
        "\n",
        "# print(\"This is list item 22....\")\n",
        "# print(adverts_description_preprocessed_str_list[22])\n",
        "\n",
        "# #print(adverts_description_preprocessed_str_list[400])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67f0152f",
      "metadata": {
        "id": "67f0152f"
      },
      "outputs": [],
      "source": [
        "corpus_original  = [str(i) for i in adverts_description_preprocessed_str_list]\n",
        "\n",
        "corpus_small = str(adverts_description_preprocessed_str_list[1000:1500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e0e7b93b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0e7b93b",
        "outputId": "6384e61b-3e00-41bb-9ae7-303f0a0d04ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000    [morris, auto, sales, valleydale, roadbirmingh...\n",
            "1001    [morris, auto, sales, valleydale, roadbirmingh...\n",
            "1002    [chevrolet, corvette, conv, offered, toy, stor...\n",
            "1003    [chevrolet, corvette, coupe, offered, toy, sto...\n",
            "1004    [chevrolet, corvette, grand, sport, offered, t...\n",
            "                              ...                        \n",
            "1495    [selling, nice, ram, laramie, mega, cab, deise...\n",
            "1496    [honda, civic, runs, drives, good, vtec, cyl, ...\n",
            "1497    [chevy, trailblazer, alloy, wheels, airbags, a...\n",
            "1498    [truckcountryautonetselling, awesome, ford, fu...\n",
            "1499    [awesome, toyota, runner, trd, loaded, low, mi...\n",
            "name: description, length: 500, dtype: object\n"
          ]
        }
      ],
      "source": [
        "#lower case the corpus\n",
        "corpus_small = corpus_small.lower()\n",
        "print(corpus_small)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c3bbb9",
      "metadata": {
        "id": "a0c3bbb9"
      },
      "source": [
        "# Clean up - digit removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "55192b81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55192b81",
        "outputId": "195b732d-845f-417d-fd6d-e88fac861ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [morris, auto, sales, valleydale, roadbirmingh...\n",
            "    [morris, auto, sales, valleydale, roadbirmingh...\n",
            "    [chevrolet, corvette, conv, offered, toy, stor...\n",
            "    [chevrolet, corvette, coupe, offered, toy, sto...\n",
            "    [chevrolet, corvette, grand, sport, offered, t...\n",
            "                              ...                        \n",
            "    [selling, nice, ram, laramie, mega, cab, deise...\n",
            "    [honda, civic, runs, drives, good, vtec, cyl, ...\n",
            "    [chevy, trailblazer, alloy, wheels, airbags, a...\n",
            "    [truckcountryautonetselling, awesome, ford, fu...\n",
            "    [awesome, toyota, runner, trd, loaded, low, mi...\n",
            "name: description, length: , dtype: object\n"
          ]
        }
      ],
      "source": [
        "# removing digits in the corpus\n",
        "import re\n",
        "corpus_small = re.sub(r'\\d+','', corpus_small)\n",
        "print(corpus_small)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca2652b7",
      "metadata": {
        "id": "ca2652b7"
      },
      "source": [
        "# Cleanup - remove punctuation and trailing spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "54bd16ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54bd16ff",
        "outputId": "2bb78caa-7084-4502-bb33-99ed11130f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    morris auto sales valleydale roadbirmingh\n",
            "    morris auto sales valleydale roadbirmingh\n",
            "    chevrolet corvette conv offered toy stor\n",
            "    chevrolet corvette coupe offered toy sto\n",
            "    chevrolet corvette grand sport offered t\n",
            "                                                      \n",
            "    selling nice ram laramie mega cab deise\n",
            "    honda civic runs drives good vtec cyl \n",
            "    chevy trailblazer alloy wheels airbags a\n",
            "    truckcountryautonetselling awesome ford fu\n",
            "    awesome toyota runner trd loaded low mi\n",
            "name description length  dtype object\n"
          ]
        }
      ],
      "source": [
        "#removing punctuations\n",
        "import string\n",
        "corpus_small = corpus_small.translate(str.maketrans('', '', string.punctuation))\n",
        "print(corpus_small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e45cc455",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "e45cc455",
        "outputId": "bf0eb76a-d14a-4e91-8f39-fab5b73dc943"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'morris auto sales valleydale roadbirmingh morris auto sales valleydale roadbirmingh chevrolet corvette conv offered toy stor chevrolet corvette coupe offered toy sto chevrolet corvette grand sport offered t selling nice ram laramie mega cab deise honda civic runs drives good vtec cyl chevy trailblazer alloy wheels airbags a truckcountryautonetselling awesome ford fu awesome toyota runner trd loaded low mi name description length dtype object'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#removing trailing whitespaces\n",
        "corpus_small = ' '.join([token for token in corpus_small.split()])\n",
        "corpus_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "374598c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "374598c4",
        "outputId": "ef13adfa-8983-4339-b5a9-9c83d78d20b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "%pip install spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf94f3fc",
      "metadata": {
        "id": "bf94f3fc"
      },
      "source": [
        "# Tokenizing the text without stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "67de3048",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67de3048",
        "outputId": "40cefeab-7d45-4ebd-9d78-66850d2b02ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK\n",
            "Tokenized corpus: ['morris', 'auto', 'sales', 'valleydale', 'roadbirmingh', 'morris', 'auto', 'sales', 'valleydale', 'roadbirmingh', 'chevrolet', 'corvette', 'conv', 'offered', 'toy', 'stor', 'chevrolet', 'corvette', 'coupe', 'offered', 'toy', 'sto', 'chevrolet', 'corvette', 'grand', 'sport', 'offered', 't', 'selling', 'nice', 'ram', 'laramie', 'mega', 'cab', 'deise', 'honda', 'civic', 'runs', 'drives', 'good', 'vtec', 'cyl', 'chevy', 'trailblazer', 'alloy', 'wheels', 'airbags', 'a', 'truckcountryautonetselling', 'awesome', 'ford', 'fu', 'awesome', 'toyota', 'runner', 'trd', 'loaded', 'low', 'mi', 'name', 'description', 'length', 'dtype', 'object']\n",
            "Tokenized corpus without stopwords: ['morris', 'auto', 'sales', 'valleydale', 'roadbirmingh', 'morris', 'auto', 'sales', 'valleydale', 'roadbirmingh', 'chevrolet', 'corvette', 'conv', 'offered', 'toy', 'stor', 'chevrolet', 'corvette', 'coupe', 'offered', 'toy', 'sto', 'chevrolet', 'corvette', 'grand', 'sport', 'offered', 'selling', 'nice', 'ram', 'laramie', 'mega', 'cab', 'deise', 'honda', 'civic', 'runs', 'drives', 'good', 'vtec', 'cyl', 'chevy', 'trailblazer', 'alloy', 'wheels', 'airbags', 'truckcountryautonetselling', 'awesome', 'ford', 'fu', 'awesome', 'toyota', 'runner', 'trd', 'loaded', 'low', 'mi', 'name', 'description', 'length', 'dtype', 'object']\n",
            "\n",
            "Spacy:\n",
            "Tokenized Corpus: ['morris', 'auto', 'sales', 'valleydale', 'roadbirmingh', 'morris', 'auto', 'sales', 'valleydale', 'roadbirmingh', 'chevrolet', 'corvette', 'conv', 'offered', 'toy', 'stor', 'chevrolet', 'corvette', 'coupe', 'offered', 'toy', 'sto', 'chevrolet', 'corvette', 'grand', 'sport', 'offered', 't', 'selling', 'nice', 'ram', 'laramie', 'mega', 'cab', 'deise', 'honda', 'civic', 'runs', 'drives', 'good', 'vtec', 'cyl', 'chevy', 'trailblazer', 'alloy', 'wheels', 'airbags', 'a', 'truckcountryautonetselling', 'awesome', 'ford', 'fu', 'awesome', 'toyota', 'runner', 'trd', 'loaded', 'low', 'mi', 'name', 'description', 'length', 'dtype', 'object']\n",
            "Tokenized corpus without stopwords ['morris', 'auto', 'sales', 'valleydale', 'roadbirmingh', 'morris', 'auto', 'sales', 'valleydale', 'roadbirmingh', 'chevrolet', 'corvette', 'conv', 'offered', 'toy', 'stor', 'chevrolet', 'corvette', 'coupe', 'offered', 'toy', 'sto', 'chevrolet', 'corvette', 'grand', 'sport', 'offered', 't', 'selling', 'nice', 'ram', 'laramie', 'mega', 'cab', 'deise', 'honda', 'civic', 'runs', 'drives', 'good', 'vtec', 'cyl', 'chevy', 'trailblazer', 'alloy', 'wheels', 'airbags', 'truckcountryautonetselling', 'awesome', 'ford', 'fu', 'awesome', 'toyota', 'runner', 'trd', 'loaded', 'low', 'mi', 'description', 'length', 'dtype', 'object']\n",
            "Difference between NLTK and spaCy output:\n",
            " {'name'}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "##NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "\n",
        "tokenized_corpus_nltk = word_tokenize(corpus_small)\n",
        "print(\"\\nNLTK\\nTokenized corpus:\",tokenized_corpus_nltk)\n",
        "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
        "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)\n",
        "\n",
        "\n",
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# import spacy\n",
        "\n",
        "#import spacy.cli\n",
        "#spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "##SPACY\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import spacy\n",
        "spacy_model = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "\n",
        "##SPACY\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import spacy\n",
        "#spacy_model = spacy.load('en_core_web_sm')\n",
        "spacy_model=nlp\n",
        "\n",
        "stopwords_spacy = spacy_model.Defaults.stop_words\n",
        "print(\"\\nSpacy:\")\n",
        "tokenized_corpus_spacy = word_tokenize(corpus_small)\n",
        "print(\"Tokenized Corpus:\", tokenized_corpus_spacy)\n",
        "tokens_without_sw= [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
        "\n",
        "print(\"Tokenized corpus without stopwords\",tokens_without_sw)\n",
        "\n",
        "\n",
        "print(\"Difference between NLTK and spaCy output:\\n\",\n",
        "      set(tokenized_corpus_without_stopwords)-set(tokens_without_sw))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef17b52",
      "metadata": {
        "id": "6ef17b52"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "19a94c4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19a94c4c",
        "outputId": "00a535be-9b5f-4660-dd1e-a3374ec4eb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Stemming:\n",
            "morris auto sales valleydale roadbirmingh morris auto sales valleydale roadbirmingh chevrolet corvette conv offered toy stor chevrolet corvette coupe offered toy sto chevrolet corvette grand sport offered t selling nice ram laramie mega cab deise honda civic runs drives good vtec cyl chevy trailblazer alloy wheels airbags a truckcountryautonetselling awesome ford fu awesome toyota runner trd loaded low mi name description length dtype object\n",
            "After Stemming:\n",
            "morri auto sale valleydal roadbirmingh morri auto sale valleydal roadbirmingh chevrolet corvett conv offer toy stor chevrolet corvett coup offer toy sto chevrolet corvett grand sport offer t sell nice ram larami mega cab deis honda civic run drive good vtec cyl chevi trailblaz alloy wheel airbag a truckcountryautonetsel awesom ford fu awesom toyota runner trd load low mi name descript length dtype object "
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer= PorterStemmer()\n",
        "\n",
        "print(\"Before Stemming:\")\n",
        "print(corpus_small)\n",
        "\n",
        "print(\"After Stemming:\")\n",
        "for word in tokenized_corpus_nltk:\n",
        "    print(stemmer.stem(word),end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757cff4f",
      "metadata": {
        "id": "757cff4f"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "70b53e99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70b53e99",
        "outputId": "3041df76-42ea-4ebd-9066-f5d6e220a646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Lemmatization:\n",
            "morris auto sales valleydale roadbirmingh morris auto sales valleydale roadbirmingh chevrolet corvette conv offered toy stor chevrolet corvette coupe offered toy sto chevrolet corvette grand sport offered t selling nice ram laramie mega cab deise honda civic runs drives good vtec cyl chevy trailblazer alloy wheels airbags a truckcountryautonetselling awesome ford fu awesome toyota runner trd loaded low mi name description length dtype object\n",
            "After Lemmatization:\n",
            "morris auto sale valleydale roadbirmingh morris auto sale valleydale roadbirmingh chevrolet corvette conv offered toy stor chevrolet corvette coupe offered toy sto chevrolet corvette grand sport offered t selling nice ram laramie mega cab deise honda civic run drive good vtec cyl chevy trailblazer alloy wheel airbags a truckcountryautonetselling awesome ford fu awesome toyota runner trd loaded low mi name description length dtype object "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "print(\"Before Lemmatization:\")\n",
        "print(corpus_small)\n",
        "\n",
        "print(\"After Lemmatization:\")\n",
        "for word in tokenized_corpus_nltk:\n",
        "    print(lemmatizer.lemmatize(word),end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b3cb60",
      "metadata": {
        "id": "45b3cb60"
      },
      "source": [
        "# POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c873e855",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c873e855",
        "outputId": "6d4e111e-4bdc-46b5-9b0d-18d132ffdd0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging using spacy:\n",
            "morris : PROPN\n",
            "auto : NOUN\n",
            "sales : NOUN\n",
            "valleydale : NOUN\n",
            "roadbirmingh : PROPN\n",
            "morris : PROPN\n",
            "auto : NOUN\n",
            "sales : NOUN\n",
            "valleydale : NOUN\n",
            "roadbirmingh : PROPN\n",
            "chevrolet : PROPN\n",
            "corvette : PROPN\n",
            "conv : PROPN\n",
            "offered : VERB\n",
            "toy : PROPN\n",
            "stor : PROPN\n",
            "chevrolet : PROPN\n",
            "corvette : PROPN\n",
            "coupe : PROPN\n",
            "offered : VERB\n",
            "toy : PROPN\n",
            "sto : PROPN\n",
            "chevrolet : PROPN\n",
            "corvette : PROPN\n",
            "grand : PROPN\n",
            "sport : NOUN\n",
            "offered : VERB\n",
            "t : NOUN\n",
            "selling : VERB\n",
            "nice : ADJ\n",
            "ram : PROPN\n",
            "laramie : PROPN\n",
            "mega : PROPN\n",
            "cab : PROPN\n",
            "deise : PROPN\n",
            "honda : PROPN\n",
            "civic : PROPN\n",
            "runs : NOUN\n",
            "drives : VERB\n",
            "good : ADJ\n",
            "vtec : ADJ\n",
            "cyl : NOUN\n",
            "chevy : NOUN\n",
            "trailblazer : NOUN\n",
            "alloy : NOUN\n",
            "wheels : NOUN\n",
            "airbags : VERB\n",
            "a : DET\n",
            "truckcountryautonetselling : VERB\n",
            "awesome : ADJ\n",
            "ford : PROPN\n",
            "fu : PROPN\n",
            "awesome : PROPN\n",
            "toyota : PROPN\n",
            "runner : PROPN\n",
            "trd : PROPN\n",
            "loaded : VERB\n",
            "low : ADJ\n",
            "mi : ADJ\n",
            "name : NOUN\n",
            "description : NOUN\n",
            "length : NOUN\n",
            "dtype : NOUN\n",
            "object : NOUN\n",
            "POS Tagging using NLTK:\n",
            "[('morris', 'JJ'),\n",
            " ('auto', 'NN'),\n",
            " ('sales', 'NNS'),\n",
            " ('valleydale', 'NN'),\n",
            " ('roadbirmingh', 'NN'),\n",
            " ('morris', 'JJ'),\n",
            " ('auto', 'NN'),\n",
            " ('sales', 'NNS'),\n",
            " ('valleydale', 'VBP'),\n",
            " ('roadbirmingh', 'JJ'),\n",
            " ('chevrolet', 'NN'),\n",
            " ('corvette', 'NN'),\n",
            " ('conv', 'NN'),\n",
            " ('offered', 'VBD'),\n",
            " ('toy', 'JJ'),\n",
            " ('stor', 'NN'),\n",
            " ('chevrolet', 'NN'),\n",
            " ('corvette', 'NN'),\n",
            " ('coupe', 'NN'),\n",
            " ('offered', 'VBD'),\n",
            " ('toy', 'JJ'),\n",
            " ('sto', 'JJ'),\n",
            " ('chevrolet', 'NN'),\n",
            " ('corvette', 'NN'),\n",
            " ('grand', 'JJ'),\n",
            " ('sport', 'NN'),\n",
            " ('offered', 'VBD'),\n",
            " ('t', 'JJ'),\n",
            " ('selling', 'NN'),\n",
            " ('nice', 'JJ'),\n",
            " ('ram', 'NN'),\n",
            " ('laramie', 'NN'),\n",
            " ('mega', 'JJ'),\n",
            " ('cab', 'NN'),\n",
            " ('deise', 'NN'),\n",
            " ('honda', 'NN'),\n",
            " ('civic', 'NN'),\n",
            " ('runs', 'VBZ'),\n",
            " ('drives', 'NNS'),\n",
            " ('good', 'JJ'),\n",
            " ('vtec', 'JJ'),\n",
            " ('cyl', 'NN'),\n",
            " ('chevy', 'NN'),\n",
            " ('trailblazer', 'NN'),\n",
            " ('alloy', 'NN'),\n",
            " ('wheels', 'NNS'),\n",
            " ('airbags', 'VBP'),\n",
            " ('a', 'DT'),\n",
            " ('truckcountryautonetselling', 'JJ'),\n",
            " ('awesome', 'JJ'),\n",
            " ('ford', 'NN'),\n",
            " ('fu', 'VBZ'),\n",
            " ('awesome', 'JJ'),\n",
            " ('toyota', 'NN'),\n",
            " ('runner', 'NN'),\n",
            " ('trd', 'NNS'),\n",
            " ('loaded', 'VBD'),\n",
            " ('low', 'JJ'),\n",
            " ('mi', 'NN'),\n",
            " ('name', 'NN'),\n",
            " ('description', 'NN'),\n",
            " ('length', 'NN'),\n",
            " ('dtype', 'NN'),\n",
            " ('object', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "\n",
        "#POS tagging using spacy\n",
        "print(\"POS Tagging using spacy:\")\n",
        "doc = spacy_model(corpus_small)\n",
        "# Token and Tag\n",
        "for token in doc:\n",
        "    print(token,\":\", token.pos_)\n",
        "\n",
        "#pos tagging using nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(\"POS Tagging using NLTK:\")\n",
        "pprint(nltk.pos_tag(word_tokenize(corpus_small)))\n",
        "type(doc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWzON6MlMe4W"
      },
      "id": "oWzON6MlMe4W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ed9bed-da05-4b9d-94c2-eeed541766e8",
      "metadata": {
        "id": "04ed9bed-da05-4b9d-94c2-eeed541766e8"
      },
      "outputs": [],
      "source": [
        "#!python -m spacy download en_core_web_md\n",
        "#import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import spacy\n",
        "\n",
        "# Load a pre-trained model that includes a text classifier\n",
        "#nlp = spacy.load(\"en_core_web_md\")  # Replace with your model that has a text classification head\n"
      ],
      "metadata": {
        "id": "fFODP0Pqsaao"
      },
      "id": "fFODP0Pqsaao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9026fbea",
      "metadata": {
        "id": "9026fbea"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m spacy download en_core_web_lg # Download the en_core_web_lg model\n",
        "#!pip install spacy-lookups-data\n",
        "#import spacy\n",
        "\n",
        "# Load a pre-trained model that includes a text classifier\n",
        "#nlp = spacy.load(\"en_core_web_md\")  # Replace with your model that has a text classification head\n",
        "#nlp = spacy.load(\"en_core_web_lg\")  # Replace with your model that has a text classification head\n"
      ],
      "metadata": {
        "id": "l3ZqNkW72iWT"
      },
      "id": "l3ZqNkW72iWT",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md  # Download the required model\n",
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "\n",
        "# Load the pre-trained model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Add the TextCategorizer to the pipeline if it's not already there\n",
        "if \"textcat\" not in nlp.pipe_names:\n",
        "    textcat = nlp.add_pipe(\"textcat\", last=True)\n",
        "else:\n",
        "    textcat = nlp.get_pipe(\"textcat\")\n",
        "\n",
        "# Define your labels\n",
        "textcat.add_label(\"LOW_PRICE\")\n",
        "textcat.add_label(\"MEDIUM_PRICE\")\n",
        "textcat.add_label(\"HIGH_PRICE\")\n",
        "textcat.add_label(\"UNKNOWN\")  # Default label for unclassified data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQs4kGLo7Tze",
        "outputId": "a9f8640a-08ff-4a19-f8ff-a83f18f3dfce"
      },
      "id": "BQs4kGLo7Tze",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Training Data with a Default Label\n",
        "# Labeled Data: Texts that belong to specific categories (LOW_PRICE, MEDIUM_PRICE, HIGH_PRICE) are labeled accordingly.\n",
        "# Unlabeled Data: Texts that don't have a specific category are labeled with UNKNOWN.\n",
        "# Sample labeled data\n",
        "train_data = [\n",
        "    (\"This car is very affordable and has low mileage.\", {\"cats\": {\"LOW_PRICE\": 1, \"MEDIUM_PRICE\": 0, \"HIGH_PRICE\": 0, \"UNKNOWN\": 0}}),\n",
        "    (\"The price is too high for this model.\", {\"cats\": {\"LOW_PRICE\": 0, \"MEDIUM_PRICE\": 0, \"HIGH_PRICE\": 1, \"UNKNOWN\": 0}}),\n",
        "\n",
        "    # Sample unclassified data with a default label\n",
        "    (\"This is a generic car review.\", {\"cats\": {\"LOW_PRICE\": 0, \"MEDIUM_PRICE\": 0, \"HIGH_PRICE\": 0, \"UNKNOWN\": 1}}),\n",
        "    (\"Another example of unclassified text.\", {\"cats\": {\"LOW_PRICE\": 0, \"MEDIUM_PRICE\": 0, \"HIGH_PRICE\": 0, \"UNKNOWN\": 1}})\n",
        "]"
      ],
      "metadata": {
        "id": "1npNDsy1SZME"
      },
      "id": "1npNDsy1SZME",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Texts to Doc Objects and Prepare for Training\n",
        "train_examples = []\n",
        "for text, annotation in train_data:\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotation)\n",
        "    train_examples.append(example)"
      ],
      "metadata": {
        "id": "OgKaYHtrSvxy"
      },
      "id": "OgKaYHtrSvxy",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-lookups-data\n",
        "#Train the model\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "# Disable other pipes during training to focus on the text categorizer\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "\n",
        "    optimizer = nlp.begin_training()\n",
        "    for epoch in range(10):  # Adjust the number of epochs as needed\n",
        "        losses = {}\n",
        "        # Shuffle and batch training data\n",
        "        batches = minibatch(train_examples, size=compounding(4.0, 32.0, 1.001))\n",
        "        for batch in batches:\n",
        "            nlp.update(batch, sgd=optimizer, drop=0.5, losses=losses)\n",
        "        print(f\"Losses at epoch {epoch}: {losses}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6bxaO3VS7HM",
        "outputId": "cb1dc4c0-eaed-4542-8e0c-81fb0b11a894"
      },
      "id": "s6bxaO3VS7HM",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy-lookups-data in /usr/local/lib/python3.10/dist-packages (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy-lookups-data) (71.0.4)\n",
            "Losses at epoch 0: {'textcat': 0.1875}\n",
            "Losses at epoch 1: {'textcat': 0.18681873381137848}\n",
            "Losses at epoch 2: {'textcat': 0.18447570502758026}\n",
            "Losses at epoch 3: {'textcat': 0.18749189376831055}\n",
            "Losses at epoch 4: {'textcat': 0.18428263068199158}\n",
            "Losses at epoch 5: {'textcat': 0.1899740844964981}\n",
            "Losses at epoch 6: {'textcat': 0.18031266331672668}\n",
            "Losses at epoch 7: {'textcat': 0.18091677129268646}\n",
            "Losses at epoch 8: {'textcat': 0.18348097801208496}\n",
            "Losses at epoch 9: {'textcat': 0.18122708797454834}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine Tune the model\n",
        "# Save the fine-tuned model\n",
        "nlp.to_disk(\"fine_tuned_car_price_model\")"
      ],
      "metadata": {
        "id": "FE7C4baWaO3A"
      },
      "id": "FE7C4baWaO3A",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and use the model\n",
        "#You can now load the model and classify new texts. The UNKNOWN label will be used for texts that don't fit the other categories:\n",
        "# Load the fine-tuned model\n",
        "# Default Label Assignment: When you have unclassified data, assign a default label (e.g., UNKNOWN) to those examples.\n",
        "# Training with Mixed Data: Train the model using both classified and unclassified examples, ensuring that the unclassified data is recognized with the default label.\n",
        "# Usage: After training, the model can recognize when a text doesn't fit any of the specific categories and classify it as UNKNOWN or another default label you define.\n",
        "# This approach ensures that your model can handle unclassified data during both training and inference.\n",
        "\n",
        "\n",
        "nlp_fine_tuned = spacy.load(\"fine_tuned_car_price_model\")\n",
        "\n",
        "# Test the model with a new Doc object\n",
        "doc = nlp_fine_tuned(\"This car's price seems reasonable given its features.\")\n",
        "print(doc.cats)  # Outputs classification scores including 'UNKNOWN'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBNAJMbuaWx4",
        "outputId": "2991569e-64be-42f1-fceb-32da228a711b"
      },
      "id": "eBNAJMbuaWx4",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'LOW_PRICE': 0.23927521705627441, 'MEDIUM_PRICE': 0.19780124723911285, 'HIGH_PRICE': 0.2368471473455429, 'UNKNOWN': 0.3260764181613922}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C1X6TBs8bZOJ"
      },
      "id": "C1X6TBs8bZOJ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"This is the doc:\")\n",
        "print(doc)\n",
        "print(\"This is the doc type:\")\n",
        "print(type(doc))\n",
        "\n",
        "# Function to model a doc as unclassified\n",
        "def mark_as_unclassified(doc):\n",
        "    doc.cats[\"LOW_PRICE\"] = 0\n",
        "    doc.cats[\"MEDIUM_PRICE\"] = 0\n",
        "    doc.cats[\"HIGH_PRICE\"] = 0\n",
        "    doc.cats[\"UNKNOWN\"] = 1\n",
        "    return doc\n",
        "\n",
        "# Apply the function to the relevant docs (replace 'your_doc' with the actual doc object)\n",
        "your_doc = nlp(\"This is the text you want to mark as unclassified.\")\n",
        "your_doc = mark_as_unclassified(your_doc)\n",
        "\n",
        "print(your_doc.cats)  # Output: {'LOW_PRICE': 0, 'MEDIUM_PRICE': 0, 'HIGH_PRICE': 0, 'UNKNOWN': 1}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNZgzaTGeAwf",
        "outputId": "903f5611-3877-4c98-b9ce-d6e2aecd8644"
      },
      "id": "LNZgzaTGeAwf",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the doc:\n",
            "This car's price seems reasonable given its features.\n",
            "This is the doc type:\n",
            "<class 'spacy.tokens.doc.Doc'>\n",
            "{'LOW_PRICE': 0, 'MEDIUM_PRICE': 0, 'HIGH_PRICE': 0, 'UNKNOWN': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_fine_tuned(\"Super car, best value ever\")\n",
        "print(doc.cats)  # Outputs classification scores including 'UNKNOWN'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dB6nB8xfV4y",
        "outputId": "3027f173-57c3-4d78-cc2d-a54e33aee672"
      },
      "id": "3dB6nB8xfV4y",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'LOW_PRICE': 0.2551645338535309, 'MEDIUM_PRICE': 0.21611487865447998, 'HIGH_PRICE': 0.24500074982643127, 'UNKNOWN': 0.28371986746788025}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_fine_tuned(\"This is the text you want to mark as unclassified.\")\n",
        "print(doc.cats)  # Outputs classification scores including 'UNKNOWN'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0Bw-Cz5fxdH",
        "outputId": "8a7755ff-120b-47eb-bd2e-a0f86406accf"
      },
      "id": "i0Bw-Cz5fxdH",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'LOW_PRICE': 0.23244164884090424, 'MEDIUM_PRICE': 0.20325390994548798, 'HIGH_PRICE': 0.24449831247329712, 'UNKNOWN': 0.31980615854263306}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataframe_adverts.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPb8AyH8LyZq",
        "outputId": "f01d24c0-af98-4cbf-ca01-1aff10aea1d4"
      },
      "id": "mPb8AyH8LyZq",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           id                                                url      region  \\\n",
            "0  7316878036  https://bham.craigslist.org/ctd/d/birmingham-2...  birmingham   \n",
            "1  7316874816  https://bham.craigslist.org/ctd/d/birmingham-2...  birmingham   \n",
            "2  7316873897  https://bham.craigslist.org/cto/d/helena-2001-...  birmingham   \n",
            "3  7316872263  https://bham.craigslist.org/ctd/d/birmingham-2...  birmingham   \n",
            "4  7316871664  https://bham.craigslist.org/ctd/d/birmingham-2...  birmingham   \n",
            "\n",
            "                    region_url  price    year manufacturer  \\\n",
            "0  https://bham.craigslist.org  21950  2012.0       toyota   \n",
            "1  https://bham.craigslist.org  13950  2011.0       toyota   \n",
            "2  https://bham.craigslist.org   5900  2001.0          NaN   \n",
            "3  https://bham.craigslist.org  12950  2005.0          bmw   \n",
            "4  https://bham.craigslist.org  18950  2010.0         ford   \n",
            "\n",
            "                       model  condition    cylinders  ... Unnamed: 30  \\\n",
            "0                 tacoma 4x4  excellent  4 cylinders  ...         NaN   \n",
            "1                     tacoma       good  4 cylinders  ...         NaN   \n",
            "2                      F-350        NaN  8 cylinders  ...         NaN   \n",
            "3                         z4  excellent  6 cylinders  ...         NaN   \n",
            "4  f150 lariat supercrew 4x4       good  8 cylinders  ...         NaN   \n",
            "\n",
            "   Unnamed: 31 Unnamed: 32 Unnamed: 33 Unnamed: 34 Unnamed: 35 Unnamed: 36  \\\n",
            "0          NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1          NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2          NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3          NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4          NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "  Unnamed: 37 Unnamed: 38 Unnamed: 39  \n",
            "0         NaN         NaN         NaN  \n",
            "1         NaN         NaN         NaN  \n",
            "2         NaN         NaN         NaN  \n",
            "3         NaN         NaN         NaN  \n",
            "4         NaN         NaN         NaN  \n",
            "\n",
            "[5 rows x 40 columns]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "class_distribution = dataframe_adverts.groupby('class_label').size()\n",
        "print(class_distribution)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XAQ-R4toIjk",
        "outputId": "d00198d4-b834-430b-b93f-8f5b38dfbcec"
      },
      "id": "4XAQ-R4toIjk",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_label\n",
            "HIGH_PRICE      279\n",
            "MEDIUM_PRICE    722\n",
            "dtype: int64\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}